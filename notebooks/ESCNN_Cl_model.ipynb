{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = datetime.date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling using average pooling\n",
    "\"\"\"\n",
    "Downsample the input array to 35 elements using interpolation.\n",
    "\"\"\"\n",
    "\n",
    "def downsample_to_35(input_array):\n",
    "    input_tensor = torch.tensor(input_array, dtype=torch.float32)\n",
    "    \n",
    "    # Reshape the input to be 1D (if it's not already)\n",
    "    if input_tensor.dim() == 1:\n",
    "        input_tensor = input_tensor.unsqueeze(0).unsqueeze(0)  # Shape (1, 1, original_length)\n",
    "    elif input_tensor.dim() == 2:\n",
    "        input_tensor = input_tensor.unsqueeze(0)  # Shape (1, original_channels, original_length)\n",
    "    \n",
    "    # Perform interpolation to downsample to 35 elements\n",
    "    downsampled_tensor = F.interpolate(input_tensor, size=35, mode='linear', align_corners=True)\n",
    "    \n",
    "    # Remove the unnecessary dimensions to return a 1D tensor\n",
    "    downsampled_array = downsampled_tensor.squeeze().numpy()\n",
    "    \n",
    "    return downsampled_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prep_data(root, keyword=None):\n",
    "\n",
    "    # Initialization of arrays\n",
    "    # Coordinates\n",
    "    x_i = [] # Initial coordinates - before downsizing\n",
    "    y_i = []\n",
    "\n",
    "    # Polars\n",
    "    alphas = []\n",
    "    Cls = []\n",
    "    Cds = []\n",
    "    Cms = []\n",
    "\n",
    "    if keyword:\n",
    "        # lists of files in each dir\n",
    "        coord_files = [f for f in os.listdir(root) if f == (keyword+'_coordinates.dat')]\n",
    "        polar_files = [f for f in os.listdir(root) if f == ('xf-'+keyword+'-il-1000000.csv')]\n",
    "    else:\n",
    "        # lists of files in each dir\n",
    "        coord_files = [f for f in os.listdir(root) if f.endswith('_coordinates.dat')]\n",
    "        polar_files = [f for f in os.listdir(root) if f.endswith('.csv')]\n",
    "\n",
    "    # Extract base names from coordinate files\n",
    "    coord_bases = {re.sub(r'\\_coordinates.dat$', '', f) for f in coord_files}\n",
    "    polar_bases = {}\n",
    "    for polar_file in polar_files:\n",
    "        match = re.match(r'xf-(.*)-il-1000000\\.csv$', polar_file)\n",
    "        if match:\n",
    "            base_name = match.group(1)\n",
    "            polar_bases[base_name] = polar_file\n",
    "    # print(polar_bases)\n",
    "    for base_name in coord_bases:\n",
    "        if base_name in polar_bases:\n",
    "            coord_file = f\"{base_name}_coordinates.dat\"\n",
    "            polar_file = polar_bases[base_name]\n",
    "\n",
    "            coordinate_data = np.loadtxt(root+coord_file)\n",
    "            # polar_data = np.loadtxt(root+polar_file, skiprows=12)\n",
    "            polar_data = pd.read_csv(root+polar_file, skiprows=10)\n",
    "            polar_data = polar_data[(polar_data['Alpha'] >= -2) & (polar_data['Alpha'] <= 10)]\n",
    "            # print(len(polar_data))\n",
    "\n",
    "            # Coordinates\n",
    "            x = []\n",
    "            y = []\n",
    "\n",
    "            # Polars\n",
    "            alpha = polar_data['Alpha'].values\n",
    "            Cl = polar_data['Cl'].values\n",
    "            Cd = polar_data['Cd'].values\n",
    "            Cm = polar_data['Cm'].values\n",
    "\n",
    "            # print(alpha)\n",
    "\n",
    "            for i in range(0, len(coordinate_data)):\n",
    "                np.array(x.append(float(coordinate_data[i][0]))) \n",
    "                np.array(y.append(float(coordinate_data[i][1])))\n",
    "                \n",
    "            if len(x) >= 35:    # Only consider the files with more than 35 coordinates\n",
    "                x_i.append(x)\n",
    "                y_i.append(y)\n",
    "\n",
    "                alphas.append(alpha)\n",
    "                # Cls.append(Cl)\n",
    "                # Cds.append(Cd)\n",
    "\n",
    "                for num_val in range(len(Cl)):\n",
    "                    Cls.append(Cl[num_val])\n",
    "                    Cds.append(Cd[num_val])\n",
    "                    Cms.append(Cm[num_val])\n",
    "\n",
    "    return x_i, y_i, Cls, Cds, Cms, alphas\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_train = \"/mnt/e/eVTOL_model/eVTOL-AirfoilModel/data/training_database\"\n",
    "x_i, y_i, Cls, Cds, Cms, alphas = prep_data(root_train)\n",
    "Cls = np.array(Cls, dtype=float)\n",
    "Cds = np.array(Cds, dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample to 35 elements\n",
    "x_f = [] # Final coordinates - after downsizing\n",
    "y_f = []\n",
    "for num_airfoil in range(0, len(x_i)):\n",
    "    downsampled_x = downsample_to_35(x_i[num_airfoil])\n",
    "    downsampled_y = downsample_to_35(y_i[num_airfoil])\n",
    "\n",
    "    x_f.append(downsampled_x)\n",
    "    y_f.append(downsampled_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrange the data in the form of elements\n",
    "* $E = [E_1, E_2, ....., E_n]$ \n",
    "* $E_1 = [x_1, y_1, x_2, y_2, \\alpha]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arange the input data in columns [x, y, alpha, Re, M]\n",
    "\n",
    "def organize_data(x_f, y_f, alphas):\n",
    "\n",
    "    Elements = []\n",
    "\n",
    "    # Loop through the polars\n",
    "    for n_file in range(len(x_f)):\n",
    "        x_temp = x_f[n_file]\n",
    "        y_temp = y_f[n_file]\n",
    "        alpha_temp = alphas[n_file]\n",
    "        \n",
    "        for j in range(len(alpha_temp)):\n",
    "            batch = []\n",
    "            # Loop through the coodrinates\n",
    "            for i in range(len(x_temp)-1):\n",
    "                element = np.array([x_temp[i], y_temp[i], x_temp[i+1], y_temp[i+1], alpha_temp[j]])\n",
    "                # Elements.append(element)\n",
    "                batch.append(element)\n",
    "            batch = np.array(batch)\n",
    "            batch = batch.flatten()\n",
    "            Elements.append(batch)\n",
    "\n",
    "    Elements = np.array(Elements)\n",
    "\n",
    "    return Elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Elements = organize_data(x_f, y_f, alphas)\n",
    "print(Elements.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ESCNN, self).__init__()\n",
    "        \n",
    "        # Conv1: Assume 1D Convolution\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=200, kernel_size=5, stride=5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        #conv2\n",
    "        self.conv2 = nn.Conv1d(in_channels=200, out_channels=200, kernel_size=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Conv3\n",
    "        self.conv3 = nn.Conv1d(in_channels=200, out_channels=100, kernel_size=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        # Conv4\n",
    "        self.conv4 = nn.Conv1d(in_channels=100, out_channels=1, kernel_size=5, padding=2)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        # Final fully connected layer to output scalar\n",
    "        self.fc1 = nn.Linear(in_features=34, out_features=34)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=34, out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reshape input if necessary, ensure it's in the shape (batch_size, channels, elements)\n",
    "        x = x.view(-1, 1, 170)  # Reshape to (batch_size, channel=1, elements=170)\n",
    "        \n",
    "        x = self.conv1(x)  \n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.conv2(x)  \n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.conv3(x)  \n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = self.conv4(x)  \n",
    "        x = self.relu4(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)  \n",
    "        x = self.relu5(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = ESCNN()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orgaize the data for the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_data = Elements\n",
    "output_data = Cds\n",
    "\n",
    "input_data_train, input_data_val, output_data_train, output_data_val = train_test_split(input_data, output_data, test_size=0.25, random_state=28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "# And move the data to cuda\n",
    "\n",
    "input_data_train = torch.from_numpy(input_data_train).float().to(device)\n",
    "input_data_val = torch.from_numpy(input_data_val).float().to(device)\n",
    "\n",
    "output_data_train = torch.from_numpy(output_data_train).float().to(device)\n",
    "output_data_val = torch.from_numpy(output_data_val).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "\n",
    "dataset_train = TensorDataset(input_data_train, output_data_train)\n",
    "dataset_val = TensorDataset(input_data_val, output_data_val)\n",
    "\n",
    "trainDataLoader = DataLoader(dataset_train, batch_size=256)\n",
    "valDataLoader = DataLoader(dataset_val, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "learning_rate = 1e-5\n",
    "weight_decay = 1e-7\n",
    "rbf_centres = 170\n",
    "num_convLayers = 4\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "# initialize a dictionary to store training history\n",
    "H = {\n",
    "\t\"train_loss\": [],\n",
    "\t\"val_loss\": []\n",
    "}\n",
    "\n",
    "# measure how long training is going to take\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "\n",
    "\n",
    "# Training loop\n",
    "\n",
    "for e in range(0, num_epochs):\n",
    "\n",
    "    # Model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # initialize total training and validation losses\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "\n",
    "    # Loop over training dataset\n",
    "    for(ip,op) in trainDataLoader:\n",
    "        # Forward pass\n",
    "        pred = model(ip)\n",
    "        pred = pred.squeeze(1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(pred, op)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # add the loss to the total training loss so far and\n",
    "        totalTrainLoss += loss\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Model to validation mode\n",
    "        model.eval()\n",
    "\n",
    "        for (ip, op) in valDataLoader:\n",
    "\n",
    "            pred = model(ip)\n",
    "            pred = pred.squeeze(1)\n",
    "            totalValLoss += criterion(pred, op)\n",
    "\n",
    "    \t\n",
    "    # Calculate the average training and validation loss\n",
    "    avgTrainLoss = totalTrainLoss / len(trainDataLoader.dataset)\n",
    "    avgValLoss = totalValLoss / len(valDataLoader.dataset)\n",
    "\n",
    "    # Update the scheduler based on the validation loss\n",
    "    scheduler.step(avgValLoss)\n",
    "\n",
    "    # Updating training history\n",
    "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "    H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "    # print the model training and validation information\n",
    "    print(\"Epoch: {}/{}\".format(e+1, num_epochs))\n",
    "    print(\"Train loss: {:.8f}\".format(avgTrainLoss))\n",
    "    print(\"Val loss: {:.8f}\\n\".format(avgValLoss))\n",
    "\n",
    "endTime = time.time()\n",
    "print(\"Finished Training\")\n",
    "print(\"[INFO] Total time taken to train the model: {:.2f}s\".format(endTime-startTime))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(np.log10(H[\"train_loss\"]), label=\"train_loss\")\n",
    "plt.plot(np.log10(H[\"val_loss\"]), label=\"val _loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "# plt.savefig('loss_curve.png')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model if needed\n",
    "# save_path = './trained_models_Cl/{}_model_Cl_ESCNN_lr{}_e{}_rbf{}_convL{}.pth'.format(date, learning_rate, num_epochs, rbf_centres,num_convLayers)\n",
    "\n",
    "# torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_test = \"/mnt/e/eVTOL_model/eVTOL-AirfoilModel/data/testing_database\"\n",
    "\n",
    "coord_files = [f for f in os.listdir(root_test) if f.endswith('_coordinates.dat')]\n",
    "coord_bases = {re.sub(r'\\_coordinates.dat$', '', f) for f in coord_files}\n",
    "\n",
    "for keyword in coord_bases:\n",
    "    x_t, y_t, Cls_t, Cms_t, Cds_t, alphas_t = prep_data(root_test, keyword)\n",
    "    Cls_t = np.array(Cls_t, dtype=float)\n",
    "    Cds_t = np.array(Cds_t, dtype=float)\n",
    "    # Cms_t = np.array(Cms_t, dtype=float)\n",
    "    \n",
    "    x_f_t = [] # Final coordinates - after downsizing\n",
    "    y_f_t = []\n",
    "    for num_airfoil in range(0, len(x_t)):\n",
    "        downsampled_x = downsample_to_35(x_t[num_airfoil])\n",
    "        downsampled_y = downsample_to_35(y_t[num_airfoil])\n",
    "\n",
    "        x_f_t.append(downsampled_x)\n",
    "        y_f_t.append(downsampled_y)\n",
    "        \n",
    "    Elements_t = organize_data(x_f_t, y_f_t, alphas_t)\n",
    "    print(\"Elements: \",Elements_t.shape)\n",
    "    print(\"Cds: \",Cls_t.shape)\n",
    "\n",
    "    if Elements_t.shape != (0,):\n",
    "        input_data_test = Elements_t\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        input_test = torch.tensor(input_data_test, dtype=torch.float32)\n",
    "\n",
    "        # Move data to GPU\n",
    "        input_test = input_test.to(device)\n",
    "\n",
    "        # Evaluate the model on test dataset\n",
    "        with torch.no_grad():\n",
    "            Cl_eval = model.forward(input_test)\n",
    "            \n",
    "        Cl_eval = Cl_eval.cpu().detach().numpy()  # Convert tensor to numpy array\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(alphas_t[0], Cl_eval)\n",
    "        plt.plot(alphas_t[0],Cds_t)\n",
    "\n",
    "        plt.legend(['NN Model', 'UIUC database'])\n",
    "        plt.title(r'$C_d$ vs $\\alpha$ for {} airfoil'.format(keyword))\n",
    "        plt.xlabel(r'AOA [$\\alpha$]')\n",
    "        plt.ylabel(r'$C_l$')\n",
    "\n",
    "    else:\n",
    "        continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
